{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPKgwBE7MJX/vSS18OEZB3O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DiyaSNS/WIDS-2025---Ask-the-Image-Explainable-Al-for-Visual-Q-A/blob/main/Image_Captioning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk tqdm seaborn opencv-python"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbSSI29FqxfS",
        "outputId": "313dc0f8-2b7e-43e0-821f-b960034f779c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (4.12.0.88)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.61.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "DgdkuyXsqLjk",
        "outputId": "0a4701e4-8cf8-41e5-9b15-c3cfde5c0d83"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸš€ Using device: cuda\n",
            "\n",
            "======================================================================\n",
            "STEP 1: DATA PREPARATION & PREPROCESSING\n",
            "======================================================================\n",
            "\n",
            "ðŸ“¥ Dataset Setup:\n",
            "Please download Flickr8k from: https://www.kaggle.com/datasets/adityajn105/flickr8k\n",
            "Expected structure:\n",
            "  flickr8k/\n",
            "    â”œâ”€â”€ Images/\n",
            "    â””â”€â”€ captions.txt\n",
            "\n",
            "Configuration:\n",
            "  img_size            : 224\n",
            "  max_length          : 24\n",
            "  vocab_size          : 10000\n",
            "  embed_dim           : 512\n",
            "  d_model             : 512\n",
            "  nhead               : 8\n",
            "  num_decoder_layers  : 4\n",
            "  dim_feedforward     : 2048\n",
            "  dropout             : 0.1\n",
            "  batch_size          : 32\n",
            "  num_epochs          : 20\n",
            "  learning_rate       : 0.0002\n",
            "\n",
            "\n",
            "======================================================================\n",
            "Loading Dataset...\n",
            "======================================================================\n",
            "\n",
            "âœ“ Dataset split:\n",
            "  Train: 800 samples\n",
            "  Val:   100 samples\n",
            "  Test:  100 samples\n",
            "\n",
            "ðŸ“š Building vocabulary...\n",
            "âœ“ Vocabulary built: 23 words\n",
            "  - Frequency threshold: 2\n",
            "  - Total unique words: 19\n",
            "  - Words in vocab: 23\n",
            "\n",
            "ðŸ“Š Dataset Statistics:\n",
            "  Caption length (mean Â± std): 5.00 Â± 1.10\n",
            "  Min length: 3\n",
            "  Max length: 6\n",
            "  OOV rate: 0.00%\n",
            "\n",
            "======================================================================\n",
            "STEP 2: CNN ENCODER\n",
            "======================================================================\n",
            "\n",
            "STEP 3: TRANSFORMER DECODER\n",
            "======================================================================\n",
            "\n",
            "STEP 4: TRAINING SETUP\n",
            "======================================================================\n",
            "âœ“ Dataloaders created\n",
            "  Train batches: 25\n",
            "  Val batches: 4\n",
            "  Test batches: 4\n",
            "\n",
            "âœ“ Model initialized\n",
            "  Total parameters: 28,330,071\n",
            "  Trainable parameters: 28,330,071\n",
            "\n",
            "======================================================================\n",
            "STEP 5: TRAINING\n",
            "======================================================================\n",
            "\n",
            "ðŸš€ Starting training...\n",
            "\n",
            "Epoch 1/5\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:05<00:00,  4.63it/s, loss=0.784]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.7842\n",
            "Val Loss: 0.2984\n",
            "Learning Rate: 0.000200\n",
            "âœ“ Best model saved!\n",
            "\n",
            "Epoch 2/5\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00,  8.53it/s, loss=0.285]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2846\n",
            "Val Loss: 0.2902\n",
            "Learning Rate: 0.000200\n",
            "âœ“ Best model saved!\n",
            "\n",
            "Epoch 3/5\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:03<00:00,  6.53it/s, loss=0.29]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2904\n",
            "Val Loss: 0.2928\n",
            "Learning Rate: 0.000200\n",
            "\n",
            "Epoch 4/5\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00,  8.92it/s, loss=0.283]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2826\n",
            "Val Loss: 0.2769\n",
            "Learning Rate: 0.000200\n",
            "âœ“ Best model saved!\n",
            "\n",
            "Epoch 5/5\n",
            "----------------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25/25 [00:02<00:00,  8.60it/s, loss=0.276]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Loss: 0.2759\n",
            "Val Loss: 0.2748\n",
            "Learning Rate: 0.000200\n",
            "âœ“ Best model saved!\n",
            "\n",
            "âœ“ Training complete!\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "STEP 6: EVALUATION\n",
            "======================================================================\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "UnpicklingError",
          "evalue": "Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.Vocabulary was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.Vocabulary])` or the `torch.serialization.safe_globals([__main__.Vocabulary])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1795735844.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    593\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;31m# Load best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 595\u001b[0;31m \u001b[0mcheckpoint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'best_model.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    596\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'model_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1527\u001b[0m                         )\n\u001b[1;32m   1528\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                         \u001b[0;32mraise\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mUnpicklingError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_get_wo_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1530\u001b[0m                 return _load(\n\u001b[1;32m   1531\u001b[0m                     \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mUnpicklingError\u001b[0m: Weights only load failed. This file can still be loaded, to do so you have two options, \u001b[1mdo those steps only if you trust the source of the checkpoint\u001b[0m. \n\t(1) In PyTorch 2.6, we changed the default value of the `weights_only` argument in `torch.load` from `False` to `True`. Re-running `torch.load` with `weights_only` set to `False` will likely succeed, but it can result in arbitrary code execution. Do it only if you got the file from a trusted source.\n\t(2) Alternatively, to load with `weights_only=True` please check the recommended steps in the following error message.\n\tWeightsUnpickler error: Unsupported global: GLOBAL __main__.Vocabulary was not an allowed global by default. Please use `torch.serialization.add_safe_globals([__main__.Vocabulary])` or the `torch.serialization.safe_globals([__main__.Vocabulary])` context manager to allowlist this global if you trust this class/function.\n\nCheck the documentation of torch.load to learn more about types accepted by default with weights_only https://pytorch.org/docs/stable/generated/torch.load.html."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import json\n",
        "import pickle\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from PIL import Image\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import cv2\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision import models\n",
        "\n",
        "# For evaluation\n",
        "from nltk.translate.bleu_score import sentence_bleu, corpus_bleu\n",
        "from nltk.translate.meteor_score import meteor_score\n",
        "import nltk\n",
        "nltk.download('wordnet', quiet=True)\n",
        "nltk.download('omw-1.4', quiet=True)\n",
        "\n",
        "# Set seeds\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"ðŸš€ Using device: {device}\\n\")\n",
        "\n",
        "# ============================================================\n",
        "# 1. DATA PREPARATION & PREPROCESSING\n",
        "# ============================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"STEP 1: DATA PREPARATION & PREPROCESSING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Download Flickr8k dataset\n",
        "print(\"\\nðŸ“¥ Dataset Setup:\")\n",
        "print(\"Please download Flickr8k from: https://www.kaggle.com/datasets/adityajn105/flickr8k\")\n",
        "print(\"Expected structure:\")\n",
        "print(\"  flickr8k/\")\n",
        "print(\"    â”œâ”€â”€ Images/\")\n",
        "print(\"    â””â”€â”€ captions.txt\")\n",
        "print()\n",
        "\n",
        "# For this demo, we'll create sample data structure\n",
        "# In production, use actual Flickr8k dataset\n",
        "DATA_DIR = \"flickr8k\"\n",
        "IMG_DIR = os.path.join(DATA_DIR, \"Images\")\n",
        "CAPTION_FILE = os.path.join(DATA_DIR, \"captions.txt\")\n",
        "\n",
        "# Configuration\n",
        "CONFIG = {\n",
        "    'img_size': 224,\n",
        "    'max_length': 24,\n",
        "    'vocab_size': 10000,\n",
        "    'embed_dim': 512,\n",
        "    'd_model': 512,\n",
        "    'nhead': 8,\n",
        "    'num_decoder_layers': 4,\n",
        "    'dim_feedforward': 2048,\n",
        "    'dropout': 0.1,\n",
        "    'batch_size': 32,\n",
        "    'num_epochs': 20,\n",
        "    'learning_rate': 2e-4,\n",
        "}\n",
        "\n",
        "print(\"Configuration:\")\n",
        "for k, v in CONFIG.items():\n",
        "    print(f\"  {k:20s}: {v}\")\n",
        "print()\n",
        "\n",
        "# ============================================================\n",
        "# 1.1 Vocabulary Builder\n",
        "# ============================================================\n",
        "\n",
        "class Vocabulary:\n",
        "    \"\"\"Build vocabulary from captions\"\"\"\n",
        "\n",
        "    def __init__(self, freq_threshold=5):\n",
        "        self.freq_threshold = freq_threshold\n",
        "        self.word2idx = {'<pad>': 0, '<bos>': 1, '<eos>': 2, '<unk>': 3}\n",
        "        self.idx2word = {0: '<pad>', 1: '<bos>', 2: '<eos>', 3: '<unk>'}\n",
        "        self.word_freq = Counter()\n",
        "\n",
        "    def build_vocabulary(self, caption_list):\n",
        "        \"\"\"Build vocabulary from list of captions\"\"\"\n",
        "        # Count word frequencies\n",
        "        for caption in caption_list:\n",
        "            tokens = self.tokenize(caption)\n",
        "            self.word_freq.update(tokens)\n",
        "\n",
        "        # Add words above threshold\n",
        "        idx = 4\n",
        "        for word, freq in self.word_freq.most_common(CONFIG['vocab_size']):\n",
        "            if freq >= self.freq_threshold:\n",
        "                self.word2idx[word] = idx\n",
        "                self.idx2word[idx] = word\n",
        "                idx += 1\n",
        "\n",
        "        print(f\"âœ“ Vocabulary built: {len(self.word2idx)} words\")\n",
        "        print(f\"  - Frequency threshold: {self.freq_threshold}\")\n",
        "        print(f\"  - Total unique words: {len(self.word_freq)}\")\n",
        "        print(f\"  - Words in vocab: {len(self.word2idx)}\")\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Simple word tokenization\"\"\"\n",
        "        return text.lower().strip().split()\n",
        "\n",
        "    def numericalize(self, text):\n",
        "        \"\"\"Convert text to indices\"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.word2idx.get(token, self.word2idx['<unk>'])\n",
        "                for token in tokens]\n",
        "\n",
        "    def decode(self, indices):\n",
        "        \"\"\"Convert indices back to text\"\"\"\n",
        "        return ' '.join([self.idx2word.get(idx, '<unk>')\n",
        "                        for idx in indices\n",
        "                        if idx not in [0, 1, 2]])  # Skip special tokens\n",
        "\n",
        "# ============================================================\n",
        "# 1.2 Dataset Class\n",
        "# ============================================================\n",
        "\n",
        "class Flickr8kDataset(Dataset):\n",
        "    \"\"\"Flickr8k Dataset for Image Captioning\"\"\"\n",
        "\n",
        "    def __init__(self, img_dir, captions_df, vocab, transform=None, max_length=24):\n",
        "        self.img_dir = img_dir\n",
        "        self.captions_df = captions_df\n",
        "        self.vocab = vocab\n",
        "        self.transform = transform\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.captions_df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.captions_df.iloc[idx]\n",
        "        img_name = row['image']\n",
        "        caption = row['caption']\n",
        "\n",
        "        # Load image\n",
        "        img_path = os.path.join(self.img_dir, img_name)\n",
        "\n",
        "        # For demo purposes - create dummy image if not exists\n",
        "        if not os.path.exists(img_path):\n",
        "            image = Image.new('RGB', (224, 224), color='gray')\n",
        "        else:\n",
        "            image = Image.open(img_path).convert('RGB')\n",
        "\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        # Numericalize caption\n",
        "        caption_vec = [self.vocab.word2idx['<bos>']]\n",
        "        caption_vec += self.vocab.numericalize(caption)\n",
        "        caption_vec += [self.vocab.word2idx['<eos>']]\n",
        "\n",
        "        # Pad or truncate\n",
        "        if len(caption_vec) > self.max_length:\n",
        "            caption_vec = caption_vec[:self.max_length]\n",
        "\n",
        "        return image, torch.tensor(caption_vec)\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Custom collate function to pad captions\"\"\"\n",
        "    images, captions = zip(*batch)\n",
        "    images = torch.stack(images, 0)\n",
        "\n",
        "    # Pad captions\n",
        "    captions = pad_sequence(captions, batch_first=True, padding_value=0)\n",
        "\n",
        "    return images, captions\n",
        "\n",
        "# ============================================================\n",
        "# 1.3 Load and Prepare Data\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"Loading Dataset...\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Create sample data for demonstration\n",
        "# In production, load from actual captions.txt\n",
        "sample_data = {\n",
        "    'image': [f'image_{i}.jpg' for i in range(1000)],\n",
        "    'caption': [\n",
        "        'a dog running in the park',\n",
        "        'people walking on the street',\n",
        "        'a cat sitting on a chair',\n",
        "        'children playing soccer',\n",
        "        'birds flying in the sky'\n",
        "    ] * 200\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(sample_data)\n",
        "\n",
        "# Split dataset\n",
        "train_size = int(0.8 * len(df))\n",
        "val_size = int(0.1 * len(df))\n",
        "\n",
        "train_df = df[:train_size].reset_index(drop=True)\n",
        "val_df = df[train_size:train_size+val_size].reset_index(drop=True)\n",
        "test_df = df[train_size+val_size:].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nâœ“ Dataset split:\")\n",
        "print(f\"  Train: {len(train_df)} samples\")\n",
        "print(f\"  Val:   {len(val_df)} samples\")\n",
        "print(f\"  Test:  {len(test_df)} samples\")\n",
        "\n",
        "# Build vocabulary\n",
        "print(\"\\nðŸ“š Building vocabulary...\")\n",
        "vocab = Vocabulary(freq_threshold=2)\n",
        "vocab.build_vocabulary(train_df['caption'].tolist())\n",
        "\n",
        "# Calculate statistics\n",
        "print(\"\\nðŸ“Š Dataset Statistics:\")\n",
        "caption_lengths = [len(vocab.tokenize(cap)) for cap in train_df['caption']]\n",
        "print(f\"  Caption length (mean Â± std): {np.mean(caption_lengths):.2f} Â± {np.std(caption_lengths):.2f}\")\n",
        "print(f\"  Min length: {np.min(caption_lengths)}\")\n",
        "print(f\"  Max length: {np.max(caption_lengths)}\")\n",
        "\n",
        "# Calculate OOV\n",
        "all_words = set()\n",
        "for cap in train_df['caption']:\n",
        "    all_words.update(vocab.tokenize(cap))\n",
        "vocab_words = set(vocab.word2idx.keys())\n",
        "oov_rate = len(all_words - vocab_words) / len(all_words) * 100\n",
        "print(f\"  OOV rate: {oov_rate:.2f}%\")\n",
        "\n",
        "# ============================================================\n",
        "# 2. CNN ENCODER\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 2: CNN ENCODER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class CNNEncoder(nn.Module):\n",
        "    \"\"\"CNN Encoder using ResNet-18 or MobileNet\"\"\"\n",
        "\n",
        "    def __init__(self, embed_dim=512, backbone='resnet18', pretrained=True):\n",
        "        super(CNNEncoder, self).__init__()\n",
        "\n",
        "        if backbone == 'resnet18':\n",
        "            resnet = models.resnet18(weights='DEFAULT' if pretrained else None)\n",
        "            self.features = nn.Sequential(*list(resnet.children())[:-2])\n",
        "            self.in_features = 512\n",
        "        elif backbone == 'mobilenet':\n",
        "            mobilenet = models.mobilenet_v2(weights='DEFAULT' if pretrained else None)\n",
        "            self.features = mobilenet.features\n",
        "            self.in_features = 1280\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown backbone: {backbone}\")\n",
        "\n",
        "        # Adaptive pooling to get fixed size\n",
        "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
        "\n",
        "        # Project to embedding dimension\n",
        "        self.projection = nn.Linear(self.in_features, embed_dim)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            images: (batch_size, 3, 224, 224)\n",
        "        Returns:\n",
        "            features: (batch_size, embed_dim)\n",
        "        \"\"\"\n",
        "        with torch.no_grad():\n",
        "            features = self.features(images)  # (B, C, H, W)\n",
        "\n",
        "        features = self.avgpool(features)  # (B, C, 1, 1)\n",
        "        features = features.flatten(1)  # (B, C)\n",
        "        features = self.projection(features)  # (B, embed_dim)\n",
        "        features = self.relu(features)\n",
        "        features = self.dropout(features)\n",
        "\n",
        "        return features\n",
        "\n",
        "# ============================================================\n",
        "# 3. TRANSFORMER DECODER\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nSTEP 3: TRANSFORMER DECODER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class TransformerDecoder(nn.Module):\n",
        "    \"\"\"Transformer Decoder for Caption Generation\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim=512, num_layers=4,\n",
        "                 nhead=8, dim_feedforward=2048, dropout=0.1):\n",
        "        super(TransformerDecoder, self).__init__()\n",
        "\n",
        "        self.embed_dim = embed_dim\n",
        "        self.vocab_size = vocab_size\n",
        "\n",
        "        # Embedding layers\n",
        "        self.word_embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
        "        self.pos_embedding = nn.Embedding(100, embed_dim)  # Max length 100\n",
        "\n",
        "        # Transformer decoder\n",
        "        decoder_layer = nn.TransformerDecoderLayer(\n",
        "            d_model=embed_dim,\n",
        "            nhead=nhead,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers)\n",
        "\n",
        "        # Output layer\n",
        "        self.fc = nn.Linear(embed_dim, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def create_masks(self, tgt):\n",
        "        \"\"\"Create causal mask for target sequence\"\"\"\n",
        "        tgt_len = tgt.size(1)\n",
        "\n",
        "        # Causal mask (prevent looking ahead)\n",
        "        causal_mask = torch.triu(\n",
        "            torch.ones(tgt_len, tgt_len, device=tgt.device) * float('-inf'),\n",
        "            diagonal=1\n",
        "        )\n",
        "\n",
        "        # Padding mask\n",
        "        tgt_padding_mask = (tgt == 0)\n",
        "\n",
        "        return causal_mask, tgt_padding_mask\n",
        "\n",
        "    def forward(self, features, captions):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            features: (batch_size, embed_dim) from encoder\n",
        "            captions: (batch_size, seq_len) target captions\n",
        "        Returns:\n",
        "            outputs: (batch_size, seq_len, vocab_size)\n",
        "        \"\"\"\n",
        "        batch_size, seq_len = captions.shape\n",
        "\n",
        "        # Embed captions\n",
        "        positions = torch.arange(seq_len, device=captions.device).unsqueeze(0).expand(batch_size, -1)\n",
        "        caption_embeds = self.word_embedding(captions) + self.pos_embedding(positions)\n",
        "        caption_embeds = self.dropout(caption_embeds)\n",
        "\n",
        "        # Prepare memory (encoder output)\n",
        "        memory = features.unsqueeze(1)  # (batch_size, 1, embed_dim)\n",
        "\n",
        "        # Create masks\n",
        "        causal_mask, tgt_padding_mask = self.create_masks(captions)\n",
        "\n",
        "        # Decoder\n",
        "        output = self.transformer_decoder(\n",
        "            tgt=caption_embeds,\n",
        "            memory=memory,\n",
        "            tgt_mask=causal_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask\n",
        "        )\n",
        "\n",
        "        # Project to vocabulary\n",
        "        output = self.fc(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "# ============================================================\n",
        "# 4. COMPLETE CAPTION MODEL\n",
        "# ============================================================\n",
        "\n",
        "class ImageCaptioningModel(nn.Module):\n",
        "    \"\"\"Complete Image Captioning Model\"\"\"\n",
        "\n",
        "    def __init__(self, vocab_size, embed_dim=512, backbone='resnet18'):\n",
        "        super(ImageCaptioningModel, self).__init__()\n",
        "\n",
        "        self.encoder = CNNEncoder(embed_dim, backbone=backbone)\n",
        "        self.decoder = TransformerDecoder(vocab_size, embed_dim)\n",
        "\n",
        "    def forward(self, images, captions):\n",
        "        features = self.encoder(images)\n",
        "        outputs = self.decoder(features, captions)\n",
        "        return outputs\n",
        "\n",
        "    def generate_caption(self, image, vocab, max_length=24):\n",
        "        \"\"\"Generate caption for a single image\"\"\"\n",
        "        self.eval()\n",
        "        with torch.no_grad():\n",
        "            # Encode image\n",
        "            features = self.encoder(image.unsqueeze(0))\n",
        "\n",
        "            # Start with <bos>\n",
        "            caption = [vocab.word2idx['<bos>']]\n",
        "\n",
        "            for _ in range(max_length):\n",
        "                caption_tensor = torch.tensor(caption).unsqueeze(0).to(image.device)\n",
        "\n",
        "                # Decode\n",
        "                output = self.decoder(features, caption_tensor)\n",
        "\n",
        "                # Get next word\n",
        "                next_word = output[0, -1].argmax().item()\n",
        "                caption.append(next_word)\n",
        "\n",
        "                # Stop at <eos>\n",
        "                if next_word == vocab.word2idx['<eos>']:\n",
        "                    break\n",
        "\n",
        "            return caption\n",
        "\n",
        "# ============================================================\n",
        "# 5. TRAINING SETUP\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\nSTEP 4: TRAINING SETUP\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Data transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((CONFIG['img_size'], CONFIG['img_size'])),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                        std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# Create datasets\n",
        "train_dataset = Flickr8kDataset(IMG_DIR, train_df, vocab, train_transform, CONFIG['max_length'])\n",
        "val_dataset = Flickr8kDataset(IMG_DIR, val_df, vocab, val_transform, CONFIG['max_length'])\n",
        "test_dataset = Flickr8kDataset(IMG_DIR, test_df, vocab, val_transform, CONFIG['max_length'])\n",
        "\n",
        "# Create dataloaders\n",
        "train_loader = DataLoader(train_dataset, batch_size=CONFIG['batch_size'],\n",
        "                         shuffle=True, collate_fn=collate_fn, num_workers=2)\n",
        "val_loader = DataLoader(val_dataset, batch_size=CONFIG['batch_size'],\n",
        "                       shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
        "test_loader = DataLoader(test_dataset, batch_size=CONFIG['batch_size'],\n",
        "                        shuffle=False, collate_fn=collate_fn, num_workers=2)\n",
        "\n",
        "print(f\"âœ“ Dataloaders created\")\n",
        "print(f\"  Train batches: {len(train_loader)}\")\n",
        "print(f\"  Val batches: {len(val_loader)}\")\n",
        "print(f\"  Test batches: {len(test_loader)}\")\n",
        "\n",
        "# Initialize model\n",
        "model = ImageCaptioningModel(len(vocab.word2idx), CONFIG['embed_dim'], backbone='resnet18')\n",
        "model = model.to(device)\n",
        "\n",
        "print(f\"\\nâœ“ Model initialized\")\n",
        "print(f\"  Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "print(f\"  Trainable parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=CONFIG['learning_rate'])\n",
        "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n",
        "\n",
        "# ============================================================\n",
        "# 6. TRAINING LOOP\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 5: TRAINING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def train_epoch(model, dataloader, criterion, optimizer):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    pbar = tqdm(dataloader, desc='Training')\n",
        "    for images, captions in pbar:\n",
        "        images = images.to(device)\n",
        "        captions = captions.to(device)\n",
        "\n",
        "        # Forward pass (teacher forcing)\n",
        "        # Input: captions[:, :-1], Target: captions[:, 1:]\n",
        "        outputs = model(images, captions[:, :-1])\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(\n",
        "            outputs.reshape(-1, outputs.size(-1)),\n",
        "            captions[:, 1:].reshape(-1)\n",
        "        )\n",
        "\n",
        "        # Backward pass\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "        pbar.set_postfix({'loss': total_loss / len(pbar)})\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "def validate(model, dataloader, criterion):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, captions in dataloader:\n",
        "            images = images.to(device)\n",
        "            captions = captions.to(device)\n",
        "\n",
        "            outputs = model(images, captions[:, :-1])\n",
        "            loss = criterion(\n",
        "                outputs.reshape(-1, outputs.size(-1)),\n",
        "                captions[:, 1:].reshape(-1)\n",
        "            )\n",
        "\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Training loop\n",
        "print(\"\\nðŸš€ Starting training...\")\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "num_epochs = 5  # Reduced for demo, use CONFIG['num_epochs'] for full training\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "    print(\"-\" * 70)\n",
        "\n",
        "    train_loss = train_epoch(model, train_loader, criterion, optimizer)\n",
        "    val_loss = validate(model, val_loader, criterion)\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "\n",
        "    print(f\"Train Loss: {train_loss:.4f}\")\n",
        "    print(f\"Val Loss: {val_loss:.4f}\")\n",
        "    print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "    # Save best model\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'val_loss': val_loss,\n",
        "            'vocab': vocab,\n",
        "        }, 'best_model.pth')\n",
        "        print(\"âœ“ Best model saved!\")\n",
        "\n",
        "    scheduler.step()\n",
        "\n",
        "print(\"\\nâœ“ Training complete!\")\n",
        "\n",
        "# Plot training curves\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_losses, label='Train Loss', marker='o')\n",
        "plt.plot(val_losses, label='Val Loss', marker='s')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.savefig('training_curves.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "# ============================================================\n",
        "# 7. EVALUATION\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 6: EVALUATION\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load best model\n",
        "checkpoint = torch.load('best_model.pth')\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "model.eval()\n",
        "\n",
        "def calculate_bleu(references, hypothesis):\n",
        "    \"\"\"Calculate BLEU-4 score\"\"\"\n",
        "    return sentence_bleu(references, hypothesis, weights=(0.25, 0.25, 0.25, 0.25))\n",
        "\n",
        "def evaluate_model(model, dataloader, vocab):\n",
        "    \"\"\"Complete evaluation with BLEU and METEOR\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    all_references = []\n",
        "    all_hypotheses = []\n",
        "    caption_lengths = []\n",
        "    repetitions = 0\n",
        "\n",
        "    print(\"\\nðŸ“Š Generating captions for evaluation...\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, captions in tqdm(dataloader):\n",
        "            images = images.to(device)\n",
        "\n",
        "            for i in range(images.size(0)):\n",
        "                # Generate caption\n",
        "                generated = model.generate_caption(images[i], vocab)\n",
        "\n",
        "                # Reference (ground truth)\n",
        "                ref_indices = captions[i].tolist()\n",
        "                ref_indices = [idx for idx in ref_indices if idx not in [0, 1, 2]]\n",
        "                reference = [vocab.idx2word.get(idx, '<unk>') for idx in ref_indices]\n",
        "\n",
        "                # Hypothesis (generated)\n",
        "                gen_indices = [idx for idx in generated if idx not in [0, 1, 2]]\n",
        "                hypothesis = [vocab.idx2word.get(idx, '<unk>') for idx in gen_indices]\n",
        "\n",
        "                all_references.append([reference])\n",
        "                all_hypotheses.append(hypothesis)\n",
        "                caption_lengths.append(len(hypothesis))\n",
        "\n",
        "                # Check for repetitions\n",
        "                if len(hypothesis) >= 3:\n",
        "                    for j in range(len(hypothesis) - 2):\n",
        "                        if hypothesis[j] == hypothesis[j+1] == hypothesis[j+2]:\n",
        "                            repetitions += 1\n",
        "                            break\n",
        "\n",
        "    # Calculate BLEU\n",
        "    bleu_scores = [calculate_bleu(ref, hyp) for ref, hyp in zip(all_references, all_hypotheses)]\n",
        "    avg_bleu = np.mean(bleu_scores)\n",
        "\n",
        "    # Calculate METEOR (on sample for speed)\n",
        "    meteor_scores = []\n",
        "    for ref, hyp in list(zip(all_references, all_hypotheses))[:100]:\n",
        "        try:\n",
        "            score = meteor_score([' '.join(ref[0])], ' '.join(hyp))\n",
        "            meteor_scores.append(score)\n",
        "        except:\n",
        "            pass\n",
        "    avg_meteor = np.mean(meteor_scores) if meteor_scores else 0\n",
        "\n",
        "    # Statistics\n",
        "    results = {\n",
        "        'bleu4': avg_bleu,\n",
        "        'meteor': avg_meteor,\n",
        "        'caption_length_mean': np.mean(caption_lengths),\n",
        "        'caption_length_std': np.std(caption_lengths),\n",
        "        'repetition_rate': repetitions / len(all_hypotheses) * 100\n",
        "    }\n",
        "\n",
        "    return results, all_references, all_hypotheses\n",
        "\n",
        "# Evaluate on test set\n",
        "results, references, hypotheses = evaluate_model(model, test_loader, vocab)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EVALUATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"BLEU-4:              {results['bleu4']:.4f}\")\n",
        "print(f\"METEOR:              {results['meteor']:.4f}\")\n",
        "print(f\"Caption length:      {results['caption_length_mean']:.2f} Â± {results['caption_length_std']:.2f}\")\n",
        "print(f\"Repetition rate:     {results['repetition_rate']:.2f}%\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================\n",
        "# 8. QUALITATIVE ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 7: QUALITATIVE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def show_examples(model, dataset, vocab, num_examples=10, title=\"Examples\"):\n",
        "    \"\"\"Show example predictions\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    indices = np.random.choice(len(dataset), num_examples, replace=False)\n",
        "\n",
        "    for idx, ax in zip(indices, axes):\n",
        "        image, caption = dataset[idx]\n",
        "\n",
        "        # Generate caption\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate_caption(image.to(device), vocab)\n",
        "\n",
        "        # Decode\n",
        "        gt_caption = vocab.decode(caption.tolist())\n",
        "        gen_caption = vocab.decode(generated)\n",
        "\n",
        "        # Denormalize image\n",
        "        img = image.permute(1, 2, 0).cpu().numpy()\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        img = std * img + mean\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(f\"GT: {gt_caption}\\nGen: {gen_caption}\", fontsize=8)\n",
        "        ax.axis('off')\n",
        "\n",
        "    plt.suptitle(title, fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    return fig\n",
        "\n",
        "# Show success examples\n",
        "fig_success = show_examples(model, test_dataset, vocab, num_examples=10,\n",
        "                           title=\"Success Examples\")\n",
        "fig_success.savefig('success_examples.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Success examples saved to success_examples.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 9. GRAD-CAM EXPLAINABILITY\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 8: EXPLAINABILITY (Grad-CAM)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "class GradCAM:\n",
        "    \"\"\"Grad-CAM for CNN Encoder\"\"\"\n",
        "\n",
        "    def __init__(self, model):\n",
        "        self.model = model\n",
        "        self.gradients = None\n",
        "        self.activations = None\n",
        "\n",
        "        # Register hooks on last conv layer\n",
        "        target_layer = self.model.encoder.features[-1]\n",
        "        target_layer.register_forward_hook(self.save_activation)\n",
        "        target_layer.register_full_backward_hook(self.save_gradient)\n",
        "\n",
        "    def save_activation(self, module, input, output):\n",
        "        self.activations = output.detach()\n",
        "\n",
        "    def save_gradient(self, module, grad_input, grad_output):\n",
        "        self.gradients = grad_output[0].detach()\n",
        "\n",
        "    def generate_cam(self, image, target_word_idx=2):  # 2 = <eos>\n",
        "        \"\"\"Generate Grad-CAM heatmap\"\"\"\n",
        "        self.model.eval()\n",
        "        image = image.unsqueeze(0).to(device)\n",
        "        image.requires_grad = True\n",
        "\n",
        "        # Forward pass\n",
        "        features = self.model.encoder(image)\n",
        "        caption = torch.tensor([[1]]).to(device)  # Start with <bos>\n",
        "        output = self.model.decoder(features, caption)\n",
        "\n",
        "        # Backward on target word\n",
        "        self.model.zero_grad()\n",
        "        target_score = output[0, 0, target_word_idx]\n",
        "        target_score.backward()\n",
        "\n",
        "        # Calculate weights\n",
        "        weights = self.gradients.mean(dim=(2, 3), keepdim=True)\n",
        "        cam = (weights * self.activations).sum(dim=1, keepdim=True)\n",
        "        cam = F.relu(cam)\n",
        "\n",
        "        # Normalize\n",
        "        cam = cam.squeeze().cpu().numpy()\n",
        "        cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-8)\n",
        "\n",
        "        return cam\n",
        "\n",
        "def apply_gradcam_overlay(image, cam, alpha=0.5):\n",
        "    \"\"\"Apply Grad-CAM overlay on image\"\"\"\n",
        "    # Denormalize image\n",
        "    img = image.permute(1, 2, 0).cpu().numpy()\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img = std * img + mean\n",
        "    img = np.clip(img, 0, 1)\n",
        "\n",
        "    # Resize CAM to image size\n",
        "    h, w = img.shape[:2]\n",
        "    cam_resized = cv2.resize(cam, (w, h))\n",
        "\n",
        "    # Apply colormap\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
        "    heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    heatmap = heatmap / 255.0\n",
        "\n",
        "    # Overlay\n",
        "    overlayed = (1 - alpha) * img + alpha * heatmap\n",
        "    overlayed = np.clip(overlayed, 0, 1)\n",
        "\n",
        "    return overlayed, heatmap\n",
        "\n",
        "# Initialize Grad-CAM\n",
        "gradcam = GradCAM(model)\n",
        "\n",
        "print(\"\\nðŸ”¥ Generating Grad-CAM visualizations...\")\n",
        "\n",
        "# Select 6 examples (3 good, 3 bad)\n",
        "fig, axes = plt.subplots(6, 3, figsize=(12, 24))\n",
        "\n",
        "for idx in range(6):\n",
        "    image, caption = test_dataset[idx]\n",
        "\n",
        "    # Generate caption\n",
        "    with torch.no_grad():\n",
        "        generated = model.generate_caption(image.to(device), vocab)\n",
        "\n",
        "    # Generate CAM\n",
        "    cam = gradcam.generate_cam(image)\n",
        "\n",
        "    # Apply overlay\n",
        "    overlayed, heatmap = apply_gradcam_overlay(image, cam)\n",
        "\n",
        "    # Denormalize original\n",
        "    img_display = image.permute(1, 2, 0).cpu().numpy()\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    img_display = std * img_display + mean\n",
        "    img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "    # Decode captions\n",
        "    gt_caption = vocab.decode(caption.tolist())\n",
        "    gen_caption = vocab.decode(generated)\n",
        "\n",
        "    # Plot\n",
        "    axes[idx, 0].imshow(img_display)\n",
        "    axes[idx, 0].set_title(f'Original\\nGT: {gt_caption[:30]}', fontsize=8)\n",
        "    axes[idx, 0].axis('off')\n",
        "\n",
        "    axes[idx, 1].imshow(heatmap)\n",
        "    axes[idx, 1].set_title('Grad-CAM', fontsize=8)\n",
        "    axes[idx, 1].axis('off')\n",
        "\n",
        "    axes[idx, 2].imshow(overlayed)\n",
        "    axes[idx, 2].set_title(f'Overlay\\nGen: {gen_caption[:30]}', fontsize=8)\n",
        "    axes[idx, 2].axis('off')\n",
        "\n",
        "plt.suptitle('Grad-CAM Explainability Analysis', fontsize=14)\n",
        "plt.tight_layout()\n",
        "plt.savefig('gradcam_analysis.png', dpi=300, bbox_inches='tight')\n",
        "plt.show()\n",
        "\n",
        "print(\"âœ“ Grad-CAM visualizations saved to gradcam_analysis.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 10. FAILURE CASE ANALYSIS\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"STEP 9: FAILURE CASE ANALYSIS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "def analyze_failures(model, dataset, vocab, num_cases=3):\n",
        "    \"\"\"Analyze failure cases\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    print(\"\\nðŸ” Analyzing mis-caption cases...\")\n",
        "\n",
        "    fig, axes = plt.subplots(num_cases, 4, figsize=(16, num_cases*4))\n",
        "    if num_cases == 1:\n",
        "        axes = axes.reshape(1, -1)\n",
        "\n",
        "    failure_analysis = []\n",
        "\n",
        "    for idx in range(num_cases):\n",
        "        image, caption = dataset[idx]\n",
        "\n",
        "        # Generate caption\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate_caption(image.to(device), vocab)\n",
        "\n",
        "        # Generate CAM\n",
        "        cam = gradcam.generate_cam(image)\n",
        "        overlayed, heatmap = apply_gradcam_overlay(image, cam)\n",
        "\n",
        "        # Denormalize\n",
        "        img_display = image.permute(1, 2, 0).cpu().numpy()\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        img_display = std * img_display + mean\n",
        "        img_display = np.clip(img_display, 0, 1)\n",
        "\n",
        "        # Decode\n",
        "        gt_caption = vocab.decode(caption.tolist())\n",
        "        gen_caption = vocab.decode(generated)\n",
        "\n",
        "        # Plot\n",
        "        axes[idx, 0].imshow(img_display)\n",
        "        axes[idx, 0].set_title(f'Case {idx+1}: Original', fontsize=10)\n",
        "        axes[idx, 0].axis('off')\n",
        "\n",
        "        axes[idx, 1].imshow(heatmap)\n",
        "        axes[idx, 1].set_title('Attention (Grad-CAM)', fontsize=10)\n",
        "        axes[idx, 1].axis('off')\n",
        "\n",
        "        axes[idx, 2].imshow(overlayed)\n",
        "        axes[idx, 2].set_title('Overlay', fontsize=10)\n",
        "        axes[idx, 2].axis('off')\n",
        "\n",
        "        axes[idx, 3].text(0.1, 0.9, f'Ground Truth:\\n{gt_caption}\\n\\nGenerated:\\n{gen_caption}',\n",
        "                         transform=axes[idx, 3].transAxes, fontsize=9,\n",
        "                         verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat'))\n",
        "        axes[idx, 3].axis('off')\n",
        "\n",
        "        # Hypothesize cause\n",
        "        hypothesis = \"Possible causes:\\n\"\n",
        "        if len(gen_caption.split()) < 5:\n",
        "            hypothesis += \"- Short caption generation\\n\"\n",
        "        if \"unk\" in gen_caption:\n",
        "            hypothesis += \"- Vocabulary gap (OOV words)\\n\"\n",
        "        if len(set(gen_caption.split())) < len(gen_caption.split()) * 0.5:\n",
        "            hypothesis += \"- Repetition issues\\n\"\n",
        "        hypothesis += \"- Domain shift or ambiguous image\\n\"\n",
        "        hypothesis += \"- Insufficient training data\"\n",
        "\n",
        "        failure_analysis.append({\n",
        "            'case': idx + 1,\n",
        "            'ground_truth': gt_caption,\n",
        "            'generated': gen_caption,\n",
        "            'hypothesis': hypothesis\n",
        "        })\n",
        "\n",
        "    plt.suptitle('Failure Case Analysis with Hypotheses', fontsize=14)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('failure_analysis.png', dpi=300, bbox_inches='tight')\n",
        "    plt.show()\n",
        "\n",
        "    return failure_analysis\n",
        "\n",
        "failure_cases = analyze_failures(model, test_dataset, vocab, num_cases=3)\n",
        "\n",
        "print(\"\\nðŸ“‹ Failure Case Hypotheses:\")\n",
        "print(\"=\"*70)\n",
        "for case in failure_cases:\n",
        "    print(f\"\\nCase {case['case']}:\")\n",
        "    print(f\"Ground Truth: {case['ground_truth']}\")\n",
        "    print(f\"Generated:    {case['generated']}\")\n",
        "    print(f\"\\n{case['hypothesis']}\")\n",
        "    print(\"-\"*70)\n",
        "\n",
        "print(\"\\nâœ“ Failure analysis saved to failure_analysis.png\")\n",
        "\n",
        "# ============================================================\n",
        "# 11. FINAL SUMMARY & REPORT\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FINAL SUMMARY\")\n",
        "print(\"=\"*70)"
      ]
    }
  ]
}